{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import neccessaire library for extracting and pre-processing the urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-24 11:58:24.066411: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-11-24 11:58:24.066435: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "[nltk_data] Downloading package stopwords to /home/triet/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/triet/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from urllib.parse import urlparse, unquote_plus\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import re\n",
    "import itertools\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from util import print_evaluation_scores\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dropout, Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "import tensorflow as tf\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract raw url into meaningful words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOPWORDS = set(stopwords.words('english') + stopwords.words('french'))\n",
    "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "REPLACE_IP_ADDRESS = re.compile(r'\\b(?:[0-9]{1,3}\\.){3}[0-9]{1,3}\\b')\n",
    "\n",
    "\n",
    "def extract_url(url):\n",
    "    \"\"\"\n",
    "    Extract URL into meaningful words\n",
    "    :param url: The web URL\n",
    "    :return: The extracted string\n",
    "    \"\"\"\n",
    "    parsed_unquoted = urlparse(unquote_plus(url))\n",
    "    text = parsed_unquoted.netloc + ' ' + parsed_unquoted.path + ' ' + parsed_unquoted.params + ' ' + parsed_unquoted.query\n",
    "    text = text.replace('\\n', ' ').lower()\n",
    "    text = REPLACE_IP_ADDRESS.sub(' ', text)\n",
    "    text = REPLACE_BY_SPACE_RE.sub(' ', text)\n",
    "    text = BAD_SYMBOLS_RE.sub(' ', text)\n",
    "    text = re.sub(\"www \", \"\", text)\n",
    "    text = ' '.join([w for w in text.split() if w not in STOPWORDS])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete rows which have only one occurence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_data(df:pd.DataFrame):\n",
    "    df = df[~df[\"url\"].duplicated()]\n",
    "    df[\"target\"] = df[\"target\"].apply(lambda x : tuple(x) if isinstance(x, (np.ndarray, list)) else x)\n",
    "    df_filtered = df.groupby(\"target\").filter(lambda x : len(x) > 1)\n",
    "    print(\"Filter complete\")\n",
    "\n",
    "    return df_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group all the labels that appear less then threshold (=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_less_occur_label(target, counter, threshold=200):\n",
    "    original_len = len(target)\n",
    "    new_target = list(label for label in target if counter[label] > threshold)\n",
    "    if len(new_target) < original_len:\n",
    "        new_target.append('0ther')\n",
    "\n",
    "    return new_target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data from the parquet file and extract url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data_dir):\n",
    "    \"\"\"\n",
    "    Preprocess data\n",
    "    :param data_dir: Path of data's directory\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    dfs = []\n",
    "    data_files = glob.glob(data_dir + \"*.snappy.parquet\")\n",
    "    for data_file in data_files:\n",
    "        df = pd.read_parquet(data_file, columns=[\"url\", \"target\"])\n",
    "        dfs.append(df)\n",
    "    dfs = pd.concat(dfs, ignore_index=True)\n",
    "    dfs[\"url\"] = dfs[\"url\"].apply(extract_url)\n",
    "    \n",
    "    return dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_28013/2259556491.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"target\"] = df[\"target\"].apply(lambda x : tuple(x) if isinstance(x, (np.ndarray, list)) else x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filter complete\n"
     ]
    }
   ],
   "source": [
    "df = filter_data(preprocess_data(\"data/\"))\n",
    "# Group and replace all labels that occur less than threshold\n",
    "counter = Counter(list(label for target in df[\"target\"].values for label in target))\n",
    "df[\"target\"] = df[\"target\"].apply(lambda x : group_less_occur_label(x, counter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (41878, 2)\n",
      "Number of distinct labels: 238\n"
     ]
    }
   ],
   "source": [
    "nb_labels = len(set(label for target in df[\"target\"].values for label in target))\n",
    "print(f\"Data shape: {df.shape}\")\n",
    "print(f\"Number of distinct labels: {nb_labels}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data to train, validation and test set. Use stratified splits because of class imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in training set: 28268\n",
      "Number of rows in validation set: 9422\n",
      "Number of rows in test set: 4188\n"
     ]
    }
   ],
   "source": [
    "test_split = 0.1\n",
    "\n",
    "# Initial train and test split.\n",
    "train_df, test_df = train_test_split(\n",
    "    df,\n",
    "    test_size=test_split,\n",
    "    stratify=df[\"target\"].values,\n",
    ")\n",
    "\n",
    "# Splitting the train set further into validation\n",
    "# and new train sets.\n",
    "val_df = train_df.sample(frac=0.25)\n",
    "train_df.drop(val_df.index, inplace=True)\n",
    "\n",
    "print(f\"Number of rows in training set: {len(train_df)}\")\n",
    "print(f\"Number of rows in validation set: {len(val_df)}\")\n",
    "print(f\"Number of rows in test set: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the actual targets from the encoded version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse_multi_hot(encoded_targets):\n",
    "    hot_indices = np.argwhere(encoded_targets==1.0)[..., 0]\n",
    "    return np.take(vocab, hot_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-hot representation of labels\n",
    "Using the StringLookup layer in tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-24 11:58:27.380652: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2021-11-24 11:58:27.380673: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2021-11-24 11:58:27.380687: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (triet-XPS): /proc/driver/nvidia/version does not exist\n",
      "2021-11-24 11:58:27.380896: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels:\n",
      "['[UNK]', '0ther', '692', '1494', '1265', '474', '907', '122', '1254', '63', '108', '1119', '184', '381', '1687', '1526', '1686', '925', '531', '1311', '572', '622', '1343', '1573', '358', '1693', '1513', '1187', '909', '408', '1077', '210', '377', '137', '294', '1107', '41', '507', '1546', '1370', '933', '1367', '1094', '1198', '1599', '61', '1721', '908', '270', '1071', '935', '1171', '1533', '1234', '1277', '1193', '937', '211', '1366', '329', '1690', '906', '1143', '1095', '1372', '1146', '34', '22', '1720', '852', '96', '1096', '401', '333', '910', '1259', '1710', '608', '920', '1192', '930', '253', '540', '378', '1692', '1368', '822', '16', '1369', '953', '1348', '1179', '1515', '1111', '1730', '1534', '1781', '1136', '3', '966', '1097', '5182', '1722', '1021', '529', '647', '1142', '5697', '1867', '1602', '105', '1264', '317', '1163', '1549', '78', '997', '374', '1538', '978', '5693', '1106', '1057', '1694', '1292', '1545', '5692', '115', '384', '1049', '1542', '1544', '1048', '5529', '394', '1266', '1556', '1410', '992', '365', '566', '1253', '1696', '617', '185', '951', '89', '104', '821', '582', '272', '353', '1273', '676', '927', '1529', '182', '865', '830', '1080', '379', '198', '1222', '1585', '5527', '357', '147', '705', '312', '244', '1135', '434', '1600', '593', '926', '146', '885', '886', '448', '343', '217', '1457', '912', '83', '191', '527', '1695', '1603', '950', '1609', '107', '98', '558', '1725', '1490', '269', '1614', '1239', '725', '309', '1310', '1488', '1365', '1789', '102', '390', '234', '1493', '914', '1044', '948', '301', '1698', '1194', '923', '816', '1043', '1093', '1155', '590', '299', '273', '271', '1601', '1173', '1145', '825', '447', '1379', '1688', '700', '1459', '305', '904', '310', '1381', '1212', '5698', '1158']\n"
     ]
    }
   ],
   "source": [
    "labels = tf.ragged.constant(df[\"target\"].values)\n",
    "lookup = tf.keras.layers.StringLookup(output_mode=\"multi_hot\")\n",
    "lookup.adapt(labels)\n",
    "vocab = lookup.get_vocabulary()\n",
    "\n",
    "print(\"Labels:\")\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example of transform a set of labels into its multi-hot version and vice-versa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original target: ['378', '1710', '5529', '96', '0ther']\n",
      "Multi hot representation: [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
      "  0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "example_target = train_df[\"target\"][0]\n",
    "print(f\"Original target: {example_target}\")\n",
    "\n",
    "multi_hot_target = lookup([example_target])\n",
    "print(f\"Multi hot representation: {multi_hot_target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding the url to have all the same length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    28268.000000\n",
       "mean        10.631739\n",
       "std          5.536528\n",
       "min          1.000000\n",
       "25%          7.000000\n",
       "50%          9.000000\n",
       "75%         13.000000\n",
       "max         71.000000\n",
       "Name: url, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[\"url\"].apply(lambda x : len(x.split(\" \"))).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 30\n",
    "padding_token = \"<pad>\"\n",
    "batch_size = 1028\n",
    "auto = tf.data.AUTOTUNE\n",
    "\n",
    "def padding(url, target):\n",
    "    # Split the given url and calculate its length\n",
    "    word_splits = tf.strings.split(url, sep=\" \")\n",
    "    url_length = tf.shape(word_splits)[0]\n",
    "\n",
    "    # Calculate the padding amount\n",
    "    padding_length = max_len - url_length\n",
    "\n",
    "    # Check if need to pad or truncate the url\n",
    "    if padding_length > 0:\n",
    "        padded_url = tf.pad([url], [[0, padding_length]], constant_values=padding_token)\n",
    "        padded_url = tf.strings.reduce_join(padded_url, separator=\"\")\n",
    "\n",
    "    else:\n",
    "        padded_url = tf.strings.reduce_join(word_splits[:max_len], separator=\" \")\n",
    "\n",
    "    # An extra dimension is needed for vectorization\n",
    "    return tf.expand_dims(padded_url, -1), target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(df, is_train=True):\n",
    "    labels = tf.ragged.constant(df[\"target\"].values)\n",
    "    multi_hot_labels = lookup(labels).numpy()\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(\n",
    "        (df[\"url\"].values, multi_hot_labels)\n",
    "    )\n",
    "    # If it is the train dataset, then shuffle it\n",
    "    dataset = dataset.shuffle(df.shape[0]) if is_train else dataset\n",
    "    # Pad the url in the dataset\n",
    "    dataset = dataset.map(padding, num_parallel_calls=auto).cache()\n",
    "\n",
    "    return dataset.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = make_dataset(train_df, is_train=True)\n",
    "test_dataset = make_dataset(test_df, is_train=False)\n",
    "val_dataset = make_dataset(val_df, is_train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1028, 1)\n",
      "Url: b'madame lefigaro fr celebrites nest fille jai elevee thomas markle deplore retrait meghan harry 190120 179219<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>'\n",
      "Target: ['0ther' '184']\n",
      "Url: b'cdiscount com sport skate shop beeper vehicule electrique drift trike rdt100 12 f 1213002 ixi3661546400876 html<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>'\n",
      "Target: ['0ther']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-24 11:58:28.723588: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "text_batch, labels_batch = next(iter(train_dataset))\n",
    "\n",
    "print(text_batch.shape)\n",
    "\n",
    "for i, url in enumerate(text_batch[:2]):\n",
    "    target = labels_batch[i].numpy()[None, ...]\n",
    "    print(f\"Url: {url[0]}\")\n",
    "    print(f\"Target: {inverse_multi_hot(target[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Tensorflow TextVectorization to calculate the tf-idf representation of url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 71\n"
     ]
    }
   ],
   "source": [
    "# Calculate the number of unique words present in our url\n",
    "vocabulary_size = train_df[\"url\"].str.split().str.len().max()\n",
    "print(f\"Vocabulary size: {vocabulary_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorizer = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=vocabulary_size,\n",
    "    ngrams=(1, 2),\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=max_len\n",
    ")\n",
    "\n",
    "# Adapt this layer to the train dataset\n",
    "with tf.device(\"/CPU:0\"):\n",
    "    text_vectorizer.adapt(train_dataset.map(lambda url, target : url))\n",
    "\n",
    "# Vectorize all the url in the train, validation and test set\n",
    "train_dataset = train_dataset.map(\n",
    "    lambda url, target : (text_vectorizer(url), target),\n",
    "    num_parallel_calls=auto\n",
    ").prefetch(auto)\n",
    "\n",
    "val_dataset = val_dataset.map(\n",
    "    lambda url, target : (text_vectorizer(url), target),\n",
    "    num_parallel_calls=auto\n",
    ").prefetch(auto)\n",
    "\n",
    "test_dataset = test_dataset.map(\n",
    "    lambda url, target : (text_vectorizer(url), target),\n",
    "    num_parallel_calls=auto\n",
    ").prefetch(auto)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the dense model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dense_model(output_shape):\n",
    "    simple_model = tf.keras.Sequential([\n",
    "        Dense(512, activation=\"relu\"),\n",
    "        Dense(256, activation=\"relu\"),\n",
    "        Dense(output_shape, activation=\"sigmoid\")\n",
    "    ])\n",
    "\n",
    "    return simple_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train dense model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "28/28 [==============================] - 1s 12ms/step - loss: 0.2927\n",
      "Epoch 2/20\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 0.1341\n",
      "Epoch 3/20\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.1005\n",
      "Epoch 4/20\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.0804\n",
      "Epoch 5/20\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.0690\n",
      "Epoch 6/20\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.0631\n",
      "Epoch 7/20\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.0600\n",
      "Epoch 8/20\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.0581\n",
      "Epoch 9/20\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.0568\n",
      "Epoch 10/20\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 0.0558\n",
      "Epoch 11/20\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.0549\n",
      "Epoch 12/20\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.0542\n",
      "Epoch 13/20\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.0535\n",
      "Epoch 14/20\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.0529\n",
      "Epoch 15/20\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.0523\n",
      "Epoch 16/20\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.0518\n",
      "Epoch 17/20\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.0514\n",
      "Epoch 18/20\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.0510\n",
      "Epoch 19/20\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.0506\n",
      "Epoch 20/20\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.0502\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "dense_model = make_dense_model(lookup.vocabulary_size())\n",
    "dense_model.compile(\n",
    "    loss=\"binary_crossentropy\",\n",
    "    optimizer=\"adam\")\n",
    "\n",
    "history = dense_model.fit(\n",
    "    train_dataset, epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate dense model on val and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.09435364041604755\n",
      "Hamming loss: 0.014144764012650886\n",
      "F1 score macro: 0.10242933402042173\n",
      "F1 score micro: 0.3926473953169095\n",
      "F1 score weighted: 0.6310514579432869\n"
     ]
    }
   ],
   "source": [
    "val_pred = dense_model.predict(val_dataset)\n",
    "val_pred = np.array(val_pred) >= 0.5\n",
    "\n",
    "val_unpack = list(val_dataset)\n",
    "y_val = tf.concat([val_unpack[i][1] for i in range(len(val_unpack))], axis=0)\n",
    "\n",
    "print_evaluation_scores(val_pred, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.09789875835721108\n",
      "Hamming loss: 0.014011940871108127\n",
      "F1 score macro: 0.10223665986990281\n",
      "F1 score micro: 0.39544807965860596\n",
      "F1 score weighted: 0.6416394237688312\n"
     ]
    }
   ],
   "source": [
    "test_pred = dense_model.predict(test_dataset)\n",
    "test_pred = np.array(test_pred) >= 0.5\n",
    "\n",
    "test_unpack = list(test_dataset)\n",
    "y_test = tf.concat([test_unpack[i][1] for i in range(len(test_unpack))], axis=0)\n",
    "\n",
    "print_evaluation_scores(test_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the dense model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_LSTM_model(input_shape, output_shape):\n",
    "    # Create an input layer \n",
    "    input_url = Input((input_shape, 1), dtype='float32')\n",
    "    # Propagate the input through a LSTM layer with 128-dimensional hidden state\n",
    "    X = LSTM(128, return_sequences=False)(input_url)\n",
    "    # Add drop out with probability of 0.5\n",
    "    X = Dropout(0.5)(X)\n",
    "    # Propagate X through a Dense layer with number of units equal number of distinct labels(output_shape)\n",
    "    X = Dense(output_shape, activation='sigmoid')(X)\n",
    "\n",
    "    # Create the model instance convert input_url to X\n",
    "    model = Model(input_url, X)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can have our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 30, 1)]           0         \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 128)               66560     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 128)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 239)               30831     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 97,391\n",
      "Trainable params: 97,391\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "28/28 [==============================] - 6s 160ms/step - loss: 0.4189\n",
      "Epoch 2/20\n",
      "28/28 [==============================] - 4s 147ms/step - loss: 0.0889\n",
      "Epoch 3/20\n",
      "28/28 [==============================] - 4s 147ms/step - loss: 0.0798\n",
      "Epoch 4/20\n",
      "28/28 [==============================] - 4s 147ms/step - loss: 0.0790\n",
      "Epoch 5/20\n",
      "28/28 [==============================] - 4s 146ms/step - loss: 0.0787\n",
      "Epoch 6/20\n",
      "28/28 [==============================] - 4s 147ms/step - loss: 0.0786\n",
      "Epoch 7/20\n",
      "28/28 [==============================] - 4s 148ms/step - loss: 0.0783\n",
      "Epoch 8/20\n",
      "28/28 [==============================] - 4s 148ms/step - loss: 0.0781\n",
      "Epoch 9/20\n",
      "28/28 [==============================] - 4s 148ms/step - loss: 0.0780\n",
      "Epoch 10/20\n",
      "28/28 [==============================] - 4s 147ms/step - loss: 0.0778\n",
      "Epoch 11/20\n",
      "28/28 [==============================] - 4s 146ms/step - loss: 0.0776\n",
      "Epoch 12/20\n",
      "28/28 [==============================] - 4s 147ms/step - loss: 0.0772\n",
      "Epoch 13/20\n",
      "28/28 [==============================] - 4s 147ms/step - loss: 0.0769\n",
      "Epoch 14/20\n",
      "28/28 [==============================] - 4s 148ms/step - loss: 0.0766\n",
      "Epoch 15/20\n",
      "28/28 [==============================] - 4s 147ms/step - loss: 0.0763\n",
      "Epoch 16/20\n",
      "28/28 [==============================] - 4s 146ms/step - loss: 0.0761\n",
      "Epoch 17/20\n",
      "28/28 [==============================] - 4s 147ms/step - loss: 0.0760\n",
      "Epoch 18/20\n",
      "28/28 [==============================] - 4s 151ms/step - loss: 0.0757\n",
      "Epoch 19/20\n",
      "28/28 [==============================] - 4s 152ms/step - loss: 0.0754\n",
      "Epoch 20/20\n",
      "28/28 [==============================] - 5s 167ms/step - loss: 0.0754\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "\n",
    "lstm_model = make_LSTM_model(max_len, lookup.vocabulary_size())\n",
    "lstm_model.summary()\n",
    "\n",
    "lstm_model.compile(\n",
    "    loss=\"binary_crossentropy\",\n",
    "    optimizer=\"adam\")\n",
    "\n",
    "history = lstm_model.fit(train_dataset, epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate dense model on val and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.06421142008066227\n",
      "Hamming loss: 0.01625635364219236\n",
      "F1 score macro: 0.0029433220502149628\n",
      "F1 score micro: 0.2183169268219769\n",
      "F1 score weighted: 0.7034539700013761\n"
     ]
    }
   ],
   "source": [
    "val_pred = lstm_model.predict(val_dataset)\n",
    "val_pred = np.array(val_pred) >= 0.5\n",
    "\n",
    "val_unpack = list(val_dataset)\n",
    "y_val = tf.concat([val_unpack[i][1] for i in range(len(val_unpack))], axis=0)\n",
    "\n",
    "print_evaluation_scores(val_pred, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.0673352435530086\n",
      "Hamming loss: 0.016174924969928027\n",
      "F1 score macro: 0.002961532354160023\n",
      "F1 score micro: 0.22081047261526615\n",
      "F1 score weighted: 0.7078062326442456\n"
     ]
    }
   ],
   "source": [
    "test_pred = lstm_model.predict(test_dataset)\n",
    "test_pred = np.array(test_pred) >= 0.5\n",
    "\n",
    "test_unpack = list(test_dataset)\n",
    "y_test = tf.concat([test_unpack[i][1] for i in range(len(test_unpack))], axis=0)\n",
    "\n",
    "print_evaluation_scores(test_pred, y_test)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "58a8da46c06026e2b0ac62635d3fd83ae6b44bbbe18250da984940292c8f0290"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
