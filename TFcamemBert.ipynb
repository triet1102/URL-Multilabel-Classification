{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TFcamemBert.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lXcPSInoQvIB"
      },
      "source": [
        "### Connect drive to google colab and installing requirements"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NPtw6SEsVsHb",
        "outputId": "3fc0ec8a-843d-4a4a-c3b0-ee4cf4538218"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "%cd /content/gdrive/MyDrive/adot_challenge\n",
        "!ls\n",
        "!pip install -r requirements.txt\n",
        "!pip install torch==1.2.0+cu92 torchvision==0.4.0+cu92 -f https://download.pytorch.org/whl/torch_stable.html"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "/content/gdrive/MyDrive/adot_challenge\n",
            "data\t\t   preprocess.py     saved_models\tTFcamemBert.py\tutil.py\n",
            "data_preprocessed  requirements.txt  TFcamemBert.ipynb\tTFmodeling.py\n",
            "Requirement already satisfied: dill==0.3.4 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 1)) (0.3.4)\n",
            "Requirement already satisfied: nltk==3.6.5 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 2)) (3.6.5)\n",
            "Requirement already satisfied: numpy==1.21.4 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 3)) (1.21.4)\n",
            "Requirement already satisfied: pandas==1.3.4 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 4)) (1.3.4)\n",
            "Requirement already satisfied: scikit_learn==1.0.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 5)) (1.0.1)\n",
            "Requirement already satisfied: tensorflow==2.7.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 6)) (2.7.0)\n",
            "Requirement already satisfied: tqdm==4.62.3 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 7)) (4.62.3)\n",
            "Requirement already satisfied: transformers==4.12.3 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 8)) (4.12.3)\n",
            "Requirement already satisfied: sentencepiece==0.1.96 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 9)) (0.1.96)\n",
            "Requirement already satisfied: fastparquet==0.7.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 10)) (0.7.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk==3.6.5->-r requirements.txt (line 2)) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk==3.6.5->-r requirements.txt (line 2)) (1.1.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk==3.6.5->-r requirements.txt (line 2)) (2021.11.10)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas==1.3.4->-r requirements.txt (line 4)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas==1.3.4->-r requirements.txt (line 4)) (2018.9)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit_learn==1.0.1->-r requirements.txt (line 5)) (3.0.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit_learn==1.0.1->-r requirements.txt (line 5)) (1.4.1)\n",
            "Requirement already satisfied: flatbuffers<3.0,>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0->-r requirements.txt (line 6)) (2.0)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0->-r requirements.txt (line 6)) (0.12.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.8,~=2.7.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0->-r requirements.txt (line 6)) (2.7.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0->-r requirements.txt (line 6)) (0.37.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0->-r requirements.txt (line 6)) (3.3.0)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0->-r requirements.txt (line 6)) (12.0.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0->-r requirements.txt (line 6)) (3.17.3)\n",
            "Requirement already satisfied: keras<2.8,>=2.7.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0->-r requirements.txt (line 6)) (2.7.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0->-r requirements.txt (line 6)) (0.2.0)\n",
            "Requirement already satisfied: tensorboard~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0->-r requirements.txt (line 6)) (2.7.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0->-r requirements.txt (line 6)) (1.42.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0->-r requirements.txt (line 6)) (3.10.0.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0->-r requirements.txt (line 6)) (1.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0->-r requirements.txt (line 6)) (1.15.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0->-r requirements.txt (line 6)) (1.1.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0->-r requirements.txt (line 6)) (1.13.3)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0->-r requirements.txt (line 6)) (3.1.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0->-r requirements.txt (line 6)) (1.6.3)\n",
            "Requirement already satisfied: gast<0.5.0,>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0->-r requirements.txt (line 6)) (0.4.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0->-r requirements.txt (line 6)) (0.22.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers==4.12.3->-r requirements.txt (line 8)) (21.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.12.3->-r requirements.txt (line 8)) (4.8.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers==4.12.3->-r requirements.txt (line 8)) (0.1.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.12.3->-r requirements.txt (line 8)) (6.0)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.12.3->-r requirements.txt (line 8)) (0.10.3)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==4.12.3->-r requirements.txt (line 8)) (0.0.46)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.12.3->-r requirements.txt (line 8)) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.12.3->-r requirements.txt (line 8)) (3.4.0)\n",
            "Requirement already satisfied: thrift>=0.11.0 in /usr/local/lib/python3.7/dist-packages (from fastparquet==0.7.1->-r requirements.txt (line 10)) (0.15.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.7/dist-packages (from fastparquet==0.7.1->-r requirements.txt (line 10)) (2021.11.0)\n",
            "Requirement already satisfied: cramjam>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from fastparquet==0.7.1->-r requirements.txt (line 10)) (2.5.0)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow==2.7.0->-r requirements.txt (line 6)) (1.5.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers==4.12.3->-r requirements.txt (line 8)) (3.0.6)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0->-r requirements.txt (line 6)) (57.4.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0->-r requirements.txt (line 6)) (0.4.6)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0->-r requirements.txt (line 6)) (1.35.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0->-r requirements.txt (line 6)) (3.3.6)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0->-r requirements.txt (line 6)) (1.8.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0->-r requirements.txt (line 6)) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0->-r requirements.txt (line 6)) (0.6.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.7.0->-r requirements.txt (line 6)) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.7.0->-r requirements.txt (line 6)) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.7.0->-r requirements.txt (line 6)) (4.7.2)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow==2.7.0->-r requirements.txt (line 6)) (1.3.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.12.3->-r requirements.txt (line 8)) (3.6.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.7.0->-r requirements.txt (line 6)) (0.4.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.12.3->-r requirements.txt (line 8)) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.12.3->-r requirements.txt (line 8)) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.12.3->-r requirements.txt (line 8)) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.12.3->-r requirements.txt (line 8)) (3.0.4)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow==2.7.0->-r requirements.txt (line 6)) (3.1.1)\n",
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Requirement already satisfied: torch==1.2.0+cu92 in /usr/local/lib/python3.7/dist-packages (1.2.0+cu92)\n",
            "Requirement already satisfied: torchvision==0.4.0+cu92 in /usr/local/lib/python3.7/dist-packages (0.4.0+cu92)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.2.0+cu92) (1.21.4)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.4.0+cu92) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from torchvision==0.4.0+cu92) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y4FVp5gccplj"
      },
      "source": [
        "from preprocess import preprocess_data\n",
        "from TFmodeling import build_camembert_model\n",
        "from util import print_evaluation_scores, get_train_test_val\n",
        "from preprocess import extract_url\n",
        "\n",
        "import torch\n",
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "import dill as pickle\n",
        "from transformers import CamembertTokenizer\n",
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hfKDKJL-g8y0"
      },
      "source": [
        "MAX_LEN = 30\n",
        "TRAIN_BATCH_SIZE = 8\n",
        "VALID_BATCH_SIZE = 4\n",
        "EPOCHS = 3\n",
        "LEARNING_RATE = 1e-05"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MjHqct2xd9bd",
        "outputId": "cbc96dbe-8fd4-4b5e-91ad-8ac0976da292"
      },
      "source": [
        "data_dir = \"data/\"\n",
        "save_data_dir = \"data_preprocessed/\"\n",
        "save_model_dir = \"saved_models/tf_camemBert\"\n",
        "\n",
        "# Preprocess data\n",
        "preprocessed_file = os.path.join(save_data_dir, \"preprocessed.pkl\")\n",
        "if not os.path.exists(preprocessed_file):\n",
        "  preprocess_data(data_dir=data_dir, save_data_dir=save_data_dir)\n",
        "\n",
        "# Read data\n",
        "with open(preprocessed_file, 'rb') as f:\n",
        "  data = pickle.load(f)\n",
        "\n",
        "# Split data into train, validation and test\n",
        "X_train, X_val, X_test, y_train, y_val, y_test = get_train_test_val(data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train_shape: (28266,)\n",
            "y_train shape: (28266, 238)\n",
            "X_test shape: (4188,)\n",
            "y_test shape: (4188, 238)\n",
            "X_val shape: (9423,)\n",
            "y_val shape: (9423, 238)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3qdf5-DXyz5-"
      },
      "source": [
        "tokenizer = CamembertTokenizer.from_pretrained(\"camembert-base\")\n",
        "X_train_tokenized = tokenizer(list(X_train), max_length=MAX_LEN, padding='max_length', truncation=True)\n",
        "X_val_tokenized = tokenizer(list(X_val), max_length=MAX_LEN, padding='max_length', truncation=True)\n",
        "X_test_tokenized = tokenizer(list(X_test), max_length=MAX_LEN, padding='max_length', truncation=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "moFLUapaPGjM",
        "outputId": "ebe178d7-fb7c-4a6f-88be-7a480c6301ac"
      },
      "source": [
        "len(y_train[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "238"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xJ_mpn9Sy2OP",
        "outputId": "e5108772-86f0-4901-af5c-a63d5c61ecc8"
      },
      "source": [
        "nb_class = len(y_train[0])\n",
        "model = build_camembert_model(nb_class=nb_class, seq_length=MAX_LEN, learning_rate=LEARNING_RATE)\n",
        "\n",
        "print('OK Setup model')\n",
        "history = model.fit({\"input_ids\": np.array(X_train_tokenized[\"input_ids\"]),\n",
        "                         \"attention_mask\": np.array(X_train_tokenized[\"attention_mask\"])},\n",
        "                        y=np.array(y_train),\n",
        "                        batch_size=TRAIN_BATCH_SIZE,\n",
        "                        epochs=EPOCHS,\n",
        "                        verbose=1)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at camembert-base were not used when initializing TFCamembertModel: ['lm_head']\n",
            "- This IS expected if you are initializing TFCamembertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFCamembertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFCamembertModel were not initialized from the model checkpoint at camembert-base and are newly initialized: ['roberta/pooler/dense/kernel:0', 'roberta/pooler/dense/bias:0']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"CamemBert\"\n",
            "__________________________________________________________________________________________________________________________________\n",
            " Layer (type)                             Output Shape                 Param #         Connected to                               \n",
            "==================================================================================================================================\n",
            " input_ids (InputLayer)                   [(None, 30)]                 0               []                                         \n",
            "                                                                                                                                  \n",
            " attention_mask (InputLayer)              [(None, 30)]                 0               []                                         \n",
            "                                                                                                                                  \n",
            " CamemBertMultilabelClassification (Camem  (None, 238)                 111395566       ['input_ids[0][0]',                        \n",
            " BertMultilabelClassification)                                                          'attention_mask[0][0]']                   \n",
            "                                                                                                                                  \n",
            "==================================================================================================================================\n",
            "Total params: 111,395,566\n",
            "Trainable params: 111,395,566\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________________________________________\n",
            "OK Setup model\n",
            "Epoch 1/3\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_camembert_model_6/roberta/pooler/dense/kernel:0', 'tf_camembert_model_6/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_camembert_model_6/roberta/pooler/dense/kernel:0', 'tf_camembert_model_6/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "3534/3534 [==============================] - 709s 195ms/step - loss: 0.1099 - acc: 0.2717\n",
            "Epoch 2/3\n",
            "3534/3534 [==============================] - 689s 195ms/step - loss: 0.0496 - acc: 0.2996\n",
            "Epoch 3/3\n",
            "3534/3534 [==============================] - 689s 195ms/step - loss: 0.0355 - acc: 0.3161\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eUHE7NV9LVXj"
      },
      "source": [
        "# Save trained model\n",
        "model.save_weights(\"saved_models/tf_camemBert/weights.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aHCibcm-RHAz"
      },
      "source": [
        "### Evaluate model on val and test sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BwYtNy-TbfLD"
      },
      "source": [
        "def model_predict(ids, mask):\n",
        "  outputs = model({\"input_ids\": ids, \"attention_mask\": mask})\n",
        "  return outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iZ3cJOcuZkDw",
        "outputId": "23100238-2ca2-4eba-99a3-e921cac4d661"
      },
      "source": [
        "val_pred = []\n",
        "val_ids_split = np.array_split(X_val_tokenized['input_ids'], 100)\n",
        "val_mask_split = np.array_split(X_val_tokenized['attention_mask'], 100)\n",
        "for ids, mask in zip(val_ids_split, val_mask_split):\n",
        "  val_pred.extend(model_predict(ids, mask))\n",
        "\n",
        "val_pred = tf.math.sigmoid(val_pred)\n",
        "val_pred = np.array(val_pred) >= 0.5\n",
        "\n",
        "print(\"Evaluation on validation set:\")\n",
        "print_evaluation_scores(y_val, val_pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation on validation set:\n",
            "Accuracy: 0.1392337896635891\n",
            "Hamming loss: 0.01048792646635222\n",
            "F1 score macro: 0.24094787687913213\n",
            "F1 score micro: 0.6101857836556788\n",
            "F1 score weighted: 0.49887306689738087\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PHboTNMCaq_P",
        "outputId": "78607fa5-35e0-451a-97de-f36469ca7fe5"
      },
      "source": [
        "test_pred = []\n",
        "test_ids_split = np.array_split(X_test_tokenized['input_ids'], 100)\n",
        "test_mask_split = np.array_split(X_test_tokenized['attention_mask'], 100)\n",
        "for ids, mask in zip(test_ids_split, test_mask_split):\n",
        "  test_pred.extend(model_predict(ids, mask))\n",
        "\n",
        "test_pred = tf.math.sigmoid(test_pred)\n",
        "test_pred = np.array(test_pred) >= 0.5\n",
        "\n",
        "print(\"Evaluation on test set:\")\n",
        "print_evaluation_scores(y_test, test_pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation on test set:\n",
            "Accuracy: 0.14207258834765998\n",
            "Hamming loss: 0.010243352355268754\n",
            "F1 score macro: 0.24402418088576847\n",
            "F1 score micro: 0.6175170450288454\n",
            "F1 score weighted: 0.5050512673823906\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gd-gZQkadaVv"
      },
      "source": [
        "### Print out some examples in test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FPw5MKrhdZDd",
        "outputId": "790c1abc-dde5-4c60-8f77-139a459c9426"
      },
      "source": [
        "for i in range(20,40):\n",
        "  print(X_test[i])\n",
        "  y_pred = [idx for idx, val in enumerate(test_pred[i]) if val == 1]\n",
        "  y_true = [idx for idx, val in enumerate(y_test[i]) if val == 1]\n",
        "  print(y_pred)\n",
        "  print(y_true)\n",
        "  print('\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "deskgram co patricialincow\n",
            "[32, 58, 227]\n",
            "[32, 51, 57, 58, 227]\n",
            "\n",
            "\n",
            "www lalanguefrancaise com dictionnaire definition mijoter\n",
            "[26, 68, 117, 128, 193]\n",
            "[26, 68, 117, 128, 193]\n",
            "\n",
            "\n",
            "annuaire 118712 fr bas rhin 67 erstein 67150 docteurs laffont grunenwald sens scm 0388986811_1e0080f00001r10400t80841g\n",
            "[19, 44, 53, 58]\n",
            "[19, 44, 53, 58, 167]\n",
            "\n",
            "\n",
            "www automobile fr voiture mercedes benz b 200 vhc car pgn 2 pgs 10 srt price sro asc ms1 17200_15_ frn 2011 ful petrol mlx 200000 ger automatic_gear itc beige dmg false\n",
            "[6, 96, 222, 228]\n",
            "[28, 58]\n",
            "\n",
            "\n",
            "www justwatch com fr serie ncis enquetes speciales saison 14\n",
            "[12, 58, 175]\n",
            "[12, 70, 122, 142, 234]\n",
            "\n",
            "\n",
            "www marinetraffic com ais home centerx 5 9 centery 45 5 zoom 8\n",
            "[58]\n",
            "[58]\n",
            "\n",
            "\n",
            "fr shopping rakuten com boutique dcz481 nav livres_litterature\n",
            "[5, 207, 218]\n",
            "[5, 65, 202, 207, 218]\n",
            "\n",
            "\n",
            "www conforama fr special canape salon sejour canape canape droit 020101 nw 124 convertible relax electrique nw 4166 revetement cuir croute cuir nw 4166 revetement 100 cuir\n",
            "[48, 75, 180]\n",
            "[48, 75, 88, 180, 210]\n",
            "\n",
            "\n",
            "www automobile fr voiture mercedes benz 63 amg coupe speedshift 7g vhc car pgn 8 pgs 10 srt price sro asc ms1 17200_198_coupe dmg false pg vipcar 293061074 html\n",
            "[6, 96, 222, 228]\n",
            "[6, 80, 96, 136, 222]\n",
            "\n",
            "\n",
            "www cdiscount com telephonie telephone mobile xiaomi redmi 2 2 16go blanc double carte double f 1440432 auc0703633593542 html\n",
            "[61, 131, 194, 204]\n",
            "[61, 85, 131, 194, 204]\n",
            "\n",
            "\n",
            "www cdiscount com high tech lecteurs enregistreurs panasonic dp ub154 lecteur blu ray uhd noir f 1062716 pan5025232889310 html\n",
            "[58]\n",
            "[58]\n",
            "\n",
            "\n",
            "www legacywholistichealth com zud posle pijavok php\n",
            "[58]\n",
            "[58]\n",
            "\n",
            "\n",
            "www cdiscount com maison meubles mobilier cen table salle manger bois frene 180 x 90 f 117600903 auc0749436122114 html\n",
            "[75]\n",
            "[27, 75, 189, 199, 210]\n",
            "\n",
            "\n",
            "euw op gg summoner username genshasagi\n",
            "[0, 42, 81, 101, 224]\n",
            "[34, 42, 101, 121, 231]\n",
            "\n",
            "\n",
            "www jeuxvideo com forums 42 30857 39692530 1 0 1 0 comment gagner plus argent jeu htm\n",
            "[0, 224]\n",
            "[65, 81, 121, 224, 231]\n",
            "\n",
            "\n",
            "www marmiton org recettes recette_tartine jambon tomates_92453 aspx\n",
            "[67, 217]\n",
            "[58, 67, 114, 147, 217]\n",
            "\n",
            "\n",
            "www cdiscount com juniors r parc bois enfant html\n",
            "[58]\n",
            "[103, 157, 158, 180, 201]\n",
            "\n",
            "\n",
            "www cdiscount com bricolage r boulonneuse choc html\n",
            "[58]\n",
            "[2, 58]\n",
            "\n",
            "\n",
            "www boiteachansons net partitions maxime forestier maitresse ecole php\n",
            "[58]\n",
            "[31, 58, 100, 216]\n",
            "\n",
            "\n",
            "www lanouvellerepublique fr vienne commune orches legendaire caillou saint martin\n",
            "[49, 58]\n",
            "[164]\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uVV5CM-hRPtL"
      },
      "source": [
        "### Test inference model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pi6-_RBP0xLl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26dcfc65-f3f5-498a-f1b5-e620366804e1"
      },
      "source": [
        "url = 'https://www.fnac.com/Apple-iPhone-12-mini-5-4-64-Go-Double-SIM-5G-Blanc/a13745982/w-4'\n",
        "url = extract_url(url)\n",
        "print(f\"Extracted url: {url}\")\n",
        "\n",
        "url_tokenized = tokenizer(url, max_length=MAX_LEN, padding='max_length', truncation=True)\n",
        "url_ids = tf.expand_dims(url_tokenized[\"input_ids\"], axis=0)\n",
        "url_mask = tf.expand_dims(url_tokenized[\"attention_mask\"], axis=0)\n",
        "\n",
        "url_outputs = model_predict(url_ids, url_mask)\n",
        "url_outputs = np.array(url_outputs) >= 0.5\n",
        "\n",
        "print(f\"Index of predicted classes: {[idx for idx, val in enumerate(url_outputs[0]) if val == 1]}\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted url: www fnac com apple iphone 12 mini 5 4 64 go double sim 5g blanc a13745982 w 4\n",
            "Index of predicted classes: [61, 131, 194, 204]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZRl3m8iMeS-"
      },
      "source": [
        "### Create and load weights for future inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4h9MnBSbMdU6",
        "outputId": "455c4cea-2e68-470a-acb3-be2cdc7538e3"
      },
      "source": [
        "new_model = build_camembert_model(nb_class=nb_class, seq_length=MAX_LEN, learning_rate=LEARNING_RATE)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at camembert-base were not used when initializing TFCamembertModel: ['lm_head']\n",
            "- This IS expected if you are initializing TFCamembertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFCamembertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFCamembertModel were not initialized from the model checkpoint at camembert-base and are newly initialized: ['roberta/pooler/dense/kernel:0', 'roberta/pooler/dense/bias:0']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"CamemBert\"\n",
            "__________________________________________________________________________________________________________________________________\n",
            " Layer (type)                             Output Shape                 Param #         Connected to                               \n",
            "==================================================================================================================================\n",
            " input_ids (InputLayer)                   [(None, 30)]                 0               []                                         \n",
            "                                                                                                                                  \n",
            " attention_mask (InputLayer)              [(None, 30)]                 0               []                                         \n",
            "                                                                                                                                  \n",
            " CamemBertMultilabelClassification (Camem  (None, 238)                 111395566       ['input_ids[0][0]',                        \n",
            " BertMultilabelClassification)                                                          'attention_mask[0][0]']                   \n",
            "                                                                                                                                  \n",
            "==================================================================================================================================\n",
            "Total params: 111,395,566\n",
            "Trainable params: 111,395,566\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lz8orE5JMkbc"
      },
      "source": [
        "new_model.load_weights(\"saved_models/tf_camemBert/weights.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}